{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import skimage\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import torchvision.transforms.functional as tf\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../train_org_img/SURPRISE/surprise7.jpg', '../train_org_img/SURPRISE/surprise6.jpg', '../train_org_img/SURPRISE/surprise4.jpg', '../train_org_img/SURPRISE/surprise5.jpg', '../train_org_img/SURPRISE/surprise1.jpg', '../train_org_img/SURPRISE/surprise2.jpg', '../train_org_img/SURPRISE/surprise3.jpg', '../train_org_img/SURPRISE/surprise11.jpg', '../train_org_img/SURPRISE/surprise10.jpg', '../train_org_img/SURPRISE/surprise12.jpg', '../train_org_img/SURPRISE/surprise13.jpg', '../train_org_img/SURPRISE/surprise17.jpg', '../train_org_img/SURPRISE/surprise16.jpg', '../train_org_img/SURPRISE/surprise28.jpg', '../train_org_img/SURPRISE/surprise14.jpg', '../train_org_img/SURPRISE/surprise15.jpg', '../train_org_img/SURPRISE/surprise30.jpg', '../train_org_img/SURPRISE/surprise24.jpg', '../train_org_img/SURPRISE/surprise18.jpg', '../train_org_img/SURPRISE/surprise19.jpg', '../train_org_img/SURPRISE/surprise25.jpg', '../train_org_img/SURPRISE/surprise27.jpg', '../train_org_img/SURPRISE/surprise26.jpg', '../train_org_img/SURPRISE/surprise22.jpg', '../train_org_img/SURPRISE/surprise23.jpg', '../train_org_img/SURPRISE/surprise20.jpg', '../train_org_img/SURPRISE/surprise8.jpg', '../train_org_img/SURPRISE/surprise9.jpg']\n"
     ]
    }
   ],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# file_name_list = glob.glob('../train_org_img/*/*')\n",
    "file_name_list = glob.glob('../train_org_img/SURPRISE/*')\n",
    "print(file_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bgimg:\n",
    "bg_list = glob.glob('bgimg/*')\n",
    "\n",
    "def rand_load_bgimg():\n",
    "    bg_idx = random.randint(0,4)\n",
    "#     bg_list.length()\n",
    "    bg_path = bg_list[bg_idx]\n",
    "#     bg_img = dlib.load_rgb_image(bg_path)\n",
    "    bg_img = Image.open(bg_path)\n",
    "#     plt.imshow(bg_img)\n",
    "    area = (0, 0, 256, 256)\n",
    "    bg_img = bg_img.crop(area)\n",
    "    bg_img = tf.to_grayscale(bg_img, num_output_channels=3)\n",
    "    return bg_img\n",
    "\n",
    "# rand_load_bgimg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transFace(face_with_mask):\n",
    "#     eval_transformer = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.RandomAffine((-180, 180), translate=(0.5,0.5), scale=(0.5,1.5))\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406),\n",
    "#                          (0.229, 0.224, 0.225)),\n",
    "#     ])  # transform it into a torch tensor\n",
    "\n",
    "    pil_with_mask = Image.fromarray(face_with_mask)\n",
    "    angle = random.randint(-180, 180)\n",
    "    scale = random.uniform(0.3,1.5)\n",
    "    translateH = random.randint(-20, 20)\n",
    "    translateV = random.randint(-20, 20)\n",
    "\n",
    "    trans_img = tf.affine(img = pil_with_mask, angle = angle, translate = (translateH, translateV), scale = scale, shear = 0)\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n",
    "    return trans_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendFace(trans_pil):\n",
    "    bg_img_pil = rand_load_bgimg()\n",
    "    trans_np = np.array(trans_pil)\n",
    "    bg_img_np = np.array(bg_img_pil)\n",
    "    bg_img_np = np.reshape(bg_img_np,(256,256,3))\n",
    "    face_mask = trans_np[:,:,2]\n",
    "    face_mask_3_dim = np.reshape(face_mask,(256,256,1))\n",
    "    bg_img_np_mask = bg_img_np * (1-face_mask_3_dim)\n",
    "    appended_img = bg_img_np_mask + trans_np\n",
    "#     bg_img_pil.paste(trans_pil)\n",
    "#     bg_img_pil.show()\n",
    "    return appended_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def getMaskedImage(file_path):\n",
    "    img = dlib.load_rgb_image(file_path)\n",
    "#     print(file_path)\n",
    "\n",
    "    emotion_tag = file_path.split(\"/\")[2]\n",
    "    filename = file_path.split(\"/\")[3].split(\".\")[0]\n",
    "    \n",
    "    rect = detector(img)[0]\n",
    "    sp = predictor(img, rect)\n",
    "    landmarks = np.array([[p.x, p.y] for p in sp.parts()])\n",
    "    outline = landmarks[[*range(17), *range(26,16,-1)]]\n",
    "    Y, X = skimage.draw.polygon(outline[:,1], outline[:,0])\n",
    "    np.clip(X, 0, 255, out = X)\n",
    "    np.clip(Y, 0, 255, out = Y)\n",
    "#     clip for out range\n",
    "    cropped_img = np.zeros(img.shape, dtype=np.uint8)\n",
    "    cropped_img[Y, X] = img[Y, X]\n",
    "    \n",
    "    img_mask = np.zeros(img.shape,dtype=np.uint8)\n",
    "    img_mask[Y,X] = 1\n",
    "#     call trans\n",
    "# SAVE TRANS IMG\n",
    "    # TODO?: save mask as [:,:,3] here?\n",
    "    cropped_img[:,:,2] = img_mask[:,:,2]\n",
    "    \n",
    "    counter = 0\n",
    "    for counter in range(5):\n",
    "        trans_img_pil = transFace(cropped_img)\n",
    "    #     Image.fromarray(trans_img).save(outimg_path)\n",
    "        pil_comb_np = appendFace(trans_img_pil)\n",
    "        pil_comb_pil = Image.fromarray(pil_comb_np)\n",
    "        br_ct = 0\n",
    "        \n",
    "        for br_ct in range(4):\n",
    "            brightness_factor = random.uniform(0,2)\n",
    "            result_img = tf.adjust_brightness(pil_comb_pil, brightness_factor)\n",
    "    #         Image.fromarray(pil_comb).save(outimg_path)\n",
    "            outimg_path = os.path.join(\"outimg\", emotion_tag, filename + \"_f\" + str(counter) + \"_br\" + str(br_ct)+ \".jpg\" )\n",
    "            result_img.save(outimg_path)\n",
    "            br_ct +=1\n",
    "            \n",
    "        counter += 1\n",
    "#     trans_img.save(pil_img)\n",
    "#     plt.imshow(img_mask[:,:,2])\n",
    "#     img_mask.shape\n",
    "#     return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for file_path in file_name_list:\n",
    "    getMaskedImage(file_path)\n",
    "#     print (file_path)\n",
    "#     croppedImg = getMaskedImage(filePath)\n",
    "# plt.imshow(croppedImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
